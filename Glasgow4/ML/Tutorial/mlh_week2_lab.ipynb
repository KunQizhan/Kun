{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<h3>Week 2: Vector/Matrix form linear regression lab</h3>\n",
    "\n",
    "\n",
    "#### Aims \n",
    "\n",
    "* Use `scikit-learn` to implement simple linear regression\n",
    "* Practice general linear regression with polynomial and RBF on the Olympic 100m data\n",
    "\n",
    "\n",
    "#### Tasks \n",
    "* Replicate Lab 1 results with `scikit-learn`\n",
    "* Rescale our data\n",
    "* Write functions to construct the design matrix, $X$, with polynomials and RBFs\n",
    "* Computing least square solutions in both \n",
    "* Solve the same problem with more sophicated gradient descent (code provided)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 1: Again, we start by loading the Olympic 100m men's data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np \n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "data = np.loadtxt('olympic100m.txt', delimiter=',') # make sure olympic100m.txt is in the right folder\n",
    "x = data[:,0][:,None] # make x a matrix\n",
    "t = data[:,1][:,None] # make t a column vector "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 2: `scikit-learn` is a widely-used library of ML algorithms. It implements linear regression as `sklearn.linear_model.LinearRegression`. Using this class, try to replicate your results from Lab Sheet 1.\n",
    "\n",
    "*Note `scikit-learn` also includes many of the other algorithms we cover on this course and is commonly used in the real world. However these lab sheets guide you to implement the methods yourself in order to achieve a better understanding. Once you know how a method works, you can better understand how to adapt and tune existing standard implementations for your specific task.*"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 3: Vector/Matrix form least square solution\n",
    "\n",
    "#### Task 3.1 Rescale $x$ \n",
    "We rescale $x$ to make it small. Doing so will stablise the computatoin, otherwise it quickly becomes unfeasible to fit polynomials over ~$2000$. Let's test the following two options:\n",
    "- Option 1: `(x-1896)/40`\n",
    "- Option 2: `(x-np.mean(x))/np.std(x) `"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x = ...# Test both options\n",
    "...# plot the rescaled data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 3.2: Check the effect of the rescaling on contour plot for simple linear model $w_0 + w_1 x$\n",
    "The rescaling shows the previously diffcult to see elliptical contours. For both rescaling options, you can use $5$ to $15$ for $w_0$, and $-2$ to $1$ for $w_1$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_candidates = ...# number of candidates\n",
    "w0_candidates = ...# generate a numpy array of possible w0 values e.g. 5 to 15\n",
    "w1_candidates = ...# generate a numpy array of possible w1 values e.g. -2 to 1\n",
    "L = np.zeros( shape = (num_candidates,num_candidates) ) # Pre-allocate the loss. We are going to have num_candidates times num_candidates of them\n",
    "\n",
    "...# Write two nested for loops to compute L\n",
    "\n",
    "plt.contour(w0_candidates, w1_candidates, L, 50) # A different way to plot contour without using meshgrid\n",
    "plt.xlabel('$w_0$')\n",
    "plt.ylabel('$w_1$')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 4.1 Write your own function to construct the design matrix with polynomials\n",
    "\n",
    "$$\\mathbf{X} = \\begin{bmatrix}\n",
    "    1       & x_{1} & x_{1}^2 & \\dots & x_{1}^K \\\\\n",
    "    1       & x_{2} & x_{2}^2 & \\dots & x_{2}^K \\\\\n",
    "    \\vdots & \\vdots &\\vdots &\\ddots &\\vdots\\\\\n",
    "    1       & x_{N} & x_{N}^2 & \\dots & x_{N}^K\n",
    "\\end{bmatrix} $$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def polynomial (x, maxorder): # The np.hstack function might be helpful\n",
    "    ... # Write you own code here\n",
    "    return X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": "#### Task 4.2 Construct the design matrix with a predefined maximum polynomial order, say $9$ ",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "maxorder = 9\n",
    "X_poly = polynomial (x, maxorder)\n",
    "X_poly.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 4.3: Compute the least square solution\n",
    "\n",
    "$$ \\widehat{\\mathbf{w}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\mathbf{X}^T \\mathbf{t} $$\n",
    "\n",
    "*You can use either your own implementation or `scikit-learn.linear_model.LinearRegression`*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def least_square(X, t):\n",
    "    ... # write your code here; if you do it 'by hand' you'll need np.linalg.solve (for matrix inversion) and np.linalg.dot\n",
    "    return w\n",
    " \n",
    "w_poly =  least_square(X_poly, t)   \n",
    "w_poly"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 4.4: Plot the fitted line\n",
    "\n",
    "To make model predictions, we need transform any $x_{new}$ with the basis function:\n",
    "$$\\mathbf{x}_{new} = \\left[\\begin{array}{c}h_0(x_{new})\\\\\\vdots\\\\h_K(x_{new})\\end{array}\\right], \\quad t_{new} = \\mathbf{x}_{new}^T \\widehat{\\mathbf{w}} $$\n",
    "\n",
    "You need to construct a new design matrix for model prediction, e.g. `polynomial(x_test, maxorder)`"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(x, t, \"ro\")\n",
    "x_test = ... # generate a separate set of x for plotting, min(x) to max(x) could be a good choice\n",
    "f_test = ... # compute the corresponding prediction by the fitted model\n",
    "plt.plot(x_test, f_test, linewidth=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Task 4.5: Linear regression with RBF\n",
    "\n",
    "Use `sklearn.kernel_ridge.KernelRidge` to perform an RBF regression, and plot the results. Set `kernel='rbf'` when creating the object to force use of an RBF kernel. `KernelRidge.fit` takes the original data as input, and constructs the design matrix $\\mathbf{X}$ internally. Note that the `alpha` parameter must be set to zero (or very small) to match what we saw in lectures; it adds an extra term to the loss, which is sometimes useful to avoid overfitting."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "...  # create a KernelRidge object and fit it to the data\n",
    "\n",
    "plt.plot(x, t, \"ro\")\n",
    "x_test = ...  # generate a separate set of x for plotting, min(x) to max(x) could be a good choice\n",
    "f_test_sklearn = ...  # predict using sklearn's kernel regression\n",
    "plt.plot(x_test, f_test_sklearn, linewidth=3)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**The remainder of Task 4 is trickier; it walks you through re-implementing the above RBF regression by hand.** It will help you to understand the mechanics of the algorithm better, but is not important for everyday use."
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "#### Task 4.6: Write your own function to construct the design matrix with one RBF kernel centered at each data point\n",
    "\n",
    "$$ h_k(x) = \\exp \\left( -\\frac{ (x-\\mbox{center}[k]) ^2}{2\\mbox{width}}  \\right)$$\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-08T19:32:52.719986Z",
     "start_time": "2025-10-08T19:32:52.715943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def rbf (x, center, width):\n",
    "    ... # write your code here\n",
    "    return X"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "#### Task 4.7 Construct  the design matrix with $x$ itself as the center parameter\n",
    "Start with `width = 10` and test different values. The result should be a `(27,27)` matrix"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "center = x\n",
    "width = 10\n",
    "X_rbf = rbf(x, center, width)"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": [
    "#### Task 4.8 Compute the least square solution with the previously defined function\n",
    "\n",
    "Also compare your results with those from `scikit-learn`"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "w_rbf = least_square(X_rbf,t)\n",
    "w_rbf"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "cell_type": "markdown",
   "source": "#### Task 4.9 Plot the fitted line"
  },
  {
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.plot(x, t, \"ro\")\n",
    "x_test = ... # generate a separate set of x for plotting, min(x) to max(x) could be a good choice\n",
    "f_test = ... # compute the corresponding prediction by the fitted model\n",
    "plt.plot(x_test, f_test, linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 4.10 (advanced)\n",
    "\n",
    "Change your (custom, not `sklearn`) implementation of `least_square` to use `np.linalg.inv` instead of `np.linalg.solve`. Does this still work? If so, which solution would you prefer? If not, why?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 5 (advanced): Instead of using the least square solution,  we test gradient descent in general linear regression setting\n",
    "\n",
    "- Average squared loss: $L(\\mathbf{w}) = \\frac{1}{N} (\\mathbf{t} - \\mathbf{X}\\mathbf{w})^T(\\mathbf{t} - \\mathbf{X}\\mathbf{w})$\n",
    "- Gradient: $ \\frac{\\partial L(\\mathbf{w})}{\\partial \\mathbf{w}} = -\\frac{2}{N} \\left(\\mathbf{X}^T\\mathbf{t} - \\mathbf{X}^T \\mathbf{X} \\mathbf{w}\\right) $"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 5.1 Define your own functions for the average squared loss and its gradient \n",
    "\n",
    "You will need to change the shape of `w` and `g`, so they fit the requirement in `scipy.optimize.minimize`. It requires the gradient (`g`) to be the shape of `(d,)`, `d` being the dimension of `w` and `g`."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def loss(w, X, t): # define the loss function\n",
    "    L = ... # the average squared loss function\n",
    "    return L\n",
    "\n",
    "def gradient(w, X, t): # define the gradient function\n",
    "    g = ...\n",
    "    return g"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-10-16T17:51:59.610838515Z",
     "start_time": "2025-10-16T17:51:59.558069937Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": "#### Task 5.2 This cell checks if your gradient function is correct by compare it with numerical approximation",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "w0 = np.ones((X_rbf.shape[1], 1))[:,0]\n",
    "\n",
    "eps    = 1e-4 # step size \n",
    "mygrad = gradient(w0, X_rbf, t)\n",
    "fdgrad = np.zeros(w0.shape)\n",
    "for d in range(len(w0)): # pertub each dimension in term\n",
    "    mask = np.zeros(w0.shape) # a binary mask that only allows selected dimension to change\n",
    "    mask[d]   = 1\n",
    "    fdgrad[d] = (loss(w0 + eps*mask, X_rbf, t) - loss(w0 - eps*mask, X_rbf, t))/(2*eps) # numerical approximation with the definition of gradient\n",
    "\n",
    "print(\"MYGRAD: \", mygrad) # my gradient output\n",
    "print(\"FDGRAD: \", fdgrad) # numerical gradient\n",
    "print(\"Error: \", np.linalg.norm(mygrad-fdgrad)/np.linalg.norm(mygrad+fdgrad) ) # error "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task 5.3: We run an advanced gradient descent method, BFGS, which automatically determine the learning rate. \n",
    "SciPy's `minimize` has already implemented a range of different methods "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "\n",
    "res = opt.minimize(loss, w0, args=(X_rbf, t), method='BFGS', jac=gradient, \n",
    "                   options={'gtol': 1e-7, 'disp': True}) # google scipy minimize for more information\n",
    "res.x # solution"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(x, t, \"ro\")\n",
    "x_test = ... # generate a separate set of x for plotting, min(x) to max(x) could be a good choice\n",
    "f_test = ... # compute the corresponding prediction by the fitted model\n",
    "plt.plot(x_test, f_test, linewidth=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-10-16T17:53:35.342886219Z",
     "start_time": "2025-10-16T17:53:35.020795175Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplt\u001B[49m\u001B[38;5;241m.\u001B[39mplot(x, t, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mro\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      2\u001B[0m x_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m \u001B[38;5;66;03m# generate a separate set of x for plotting, min(x) to max(x) could be a good choice\u001B[39;00m\n\u001B[1;32m      3\u001B[0m f_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m \u001B[38;5;66;03m# compute the corresponding prediction by the fitted model\u001B[39;00m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'plt' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loss at the least square solution is still lower"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss(w_rbf[:,0], X_rbf, t) < loss(res.x, X_rbf, t)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
