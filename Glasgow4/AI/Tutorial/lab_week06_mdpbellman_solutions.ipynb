{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - COMPSCI4004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Week 6: Sequential decision-making under uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>SOLUTIONS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose/Learning objectives:**\n",
    "* Get hands-on experience with sequential decision-making in uncertain scenarios/environments\n",
    "* Formulate and define MDPs for realistic problems (in Python)\n",
    "* Solve MDPs using Value Iteration and find a policy using MEU to better understand what the Bellman equations are and what they can be used for.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Guide:** The notebook contains specific tasks you'll need to carry out to make the notebook run. These are indicated with::\n",
    "\n",
    "* <font color=dark-magenta>TASK:</font> This is a task for you to carry out before proceeding\n",
    "* <font color=green>CHECKPOINT:</font> This indicates a key point you should understand before proceeding\n",
    "\n",
    "It is important that you reach question Q6.4 and experiment with different transition models and reward functions. If you're in a hurry and already comfortable with MDPs, you can jump to Q6.2.\n",
    "\n",
    "**Notice:** This notebook is comprehensive and will most likely take you 3-4h to complete; this is expected (a solution will be provided a few days after the lab session)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.0 Introduction & Housekeeping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we explore how to make decisions in uncertain domains using techniques from probability and utility theory, with a particular focus on sequential decisions using Markov Decision Processes (MDPs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.0.1 Prerequisites\n",
    "\n",
    "You will need to import a few basic models from the `AIMA toolbox`. Importing the modules should happen without error. If you get an error you probably need to update your Python installation with the modules causing the issue.\n",
    "\n",
    "NOTE: If you get an error due to ipywidgets, then either try to install/update the module or comment them out and do the exercise without the visualization part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.7 | packaged by Anaconda, Inc. | (main, Dec 15 2023, 18:05:47) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIMA_TOOLBOX_ROOT=\"C:/Users/Edmond Ho/OneDrive - University of Glasgow/Course/AI/2024-25/aima-python-uofg_v20212022a\"\n",
    "sys.path.append(AIMA_TOOLBOX_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aimautils import argmax, vector_add, print_table\n",
    "from grid import orientations, turn_right, turn_left # note grid.py is provided in the zip file of the lab materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.0.2 Sequential Decisions - Markov decision processes (MDPs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The real world is uncertain but we as humans still manage to make decisions every day. Probability and utility theory can help us design artificial systems which can cope with certain types of uncertainty, i.e. MDPs help us deal with fully observable and non-deterministic/stochastic environments. For dealing with partially observable and stochastic cases we make use of the generalization of MDPs named POMDPs (partially observable Markov decision process) which we will not consider in this session.\n",
    "\n",
    "- A stochastic process has the **Markov property** if the conditional probability distribution of future states of the process (conditional on both past and present states) depends only upon the present state, not on the sequence of events that preceded it. In our case it is the transitions model which has this property, i.e. the next depends only on the previous one, not the full history e.g. $P(s_{3}|s_2,a)$ and not $P(s_{3}|s_2,s_1,s_0,a)$\n",
    "\n",
    "- The overall reason for solving an MDP is to come up with a policy that guides us in selecting the best action in each state so we maximize the expected sum of future rewards given that the transition model represents uncertainty in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.1 Basics: Implementing and representing an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Note:*** you can skip this part if you are already comfortable with MDPs (goto Q6.2-Q.7.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.1.1 Representing an MDP in Python \n",
    "\n",
    "* <font color=dark-magenta>TASK:</font> Inspect the following code which implements a class for representing an MDP in Python. This is still a rather empty class but you should see that the class can represent all the mathematical objects we need to define an MDP. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text.  Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs.  We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, gamma=.9):\n",
    "        self.init = init\n",
    "        self.actlist = actlist\n",
    "        self.terminals = terminals\n",
    "        if not (0 <= gamma < 1):\n",
    "            raise ValueError(\"An MDP must have 0 <= gamma < 1\")\n",
    "        self.gamma = gamma\n",
    "        self.states = set()\n",
    "        self.reward = {}\n",
    "\n",
    "    def R(self, state):\n",
    "        \"Return a numeric reward for this state.\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model.  From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Set of actions that can be performed in this state.  By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **_ _init_ _** method takes in the following parameters:\n",
    "\n",
    "    - init: the initial state.\n",
    "    - actlist: List of actions possible in each state.\n",
    "    - terminals: List of terminal states where the only possible action is exit\n",
    "    - gamma: Discounting factor. This makes sure that delayed rewards have less value compared to immediate ones.\n",
    "\n",
    "- **R** method returns the reward for each state by using the self.reward dict.\n",
    "\n",
    "- **T** method is not implemented and is somewhat different from the text. Here we return (probability, s') pairs where s' belongs to the list of possible states by taking action a in state s.\n",
    "\n",
    "- **actions** method returns the list of actions possible in each state. By default, it returns all actions for states other than terminal states.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.1.2 A Simple MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get experience with reading an MDP from a state-space representation with decision nodes, we'll consider the following state-space / action graph: \n",
    "<img src=\"./resources/mdp-b.png\">\n",
    "\n",
    "<font color=dark-magenta>TASK:</font>  From the figure above;\n",
    "* Identify and list the states\n",
    "* Identify and list the actions possible in each state, i.e., A(s)\n",
    "* Identify and list the rewards, i.e. R(s)\n",
    "* Determine the transition model, i.e. write down (on paper or in the notebook) a list or table with the relevant probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "States: A,B,C (note: the notation is not ideal here usign capital letters to denote the outcome of a random variable S but you will see this from time to time)\n",
    "\n",
    "Actions: \n",
    "- $Actions(S=A)=\\{X,Y\\}$\n",
    "- $Actions(S=B)=\\{X,Y\\}$\n",
    "- $Actions(S=End)=$Ã˜ (it is a terminal state)\n",
    "\n",
    "Rewards:\n",
    "- $R(S=A)$ = 5\n",
    "- $R(S=B)$ = -10\n",
    "- $R(S=C)$ = 100\n",
    "\n",
    "Transition model - we don't need to worry about what happends in \"End\" as the process stops here:\n",
    "\n",
    "- $P(S_{t+1}=A|S_t=A, Action=X)=0.3$\n",
    "\n",
    "- $P(S_{t+1}=B|S_t=A, Action=X)=0.7$\n",
    "\n",
    "- $P(S_{t+1}=A|S_t=A, Action=Y)=1.0$\n",
    "\n",
    "- $P(S_{t+1}=B|S_t=A, Action=Y)=0.0$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $P(S_{t+1}=B | S_t=B, Action=X)=0.2$\n",
    "\n",
    "- $P(S_{t+1}=End  |S_t=B, Action=X)=0.8$\n",
    "\n",
    "- $P(S_{t+1}=A | S_t=B, Action=Y)=1.0$\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.1.3 Representing the elements of an MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we show you how to represent the elements of an MDP in Python (using the AIMA toolbox).\n",
    "- <font color=dark-magenta>TASK:</font> You'll now need to fill in the missing elements yourself with the values you have identified in Q6.1.2 (look for \"TODO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Transition Matrix as nested dict. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# State -> Actions in state -> States by each action -> Probabilty\u001b[39;00m\n\u001b[0;32m      4\u001b[0m t \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      6\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0.7\u001b[39m},\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1.0\u001b[39m}            \n\u001b[0;32m      8\u001b[0m          },\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[1;32m---> 10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: TODO,            \n\u001b[0;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m1.0\u001b[39m}\n\u001b[0;32m     12\u001b[0m          },\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[0;32m     14\u001b[0m }\n\u001b[0;32m     16\u001b[0m init \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# initial state\u001b[39;00m\n\u001b[0;32m     18\u001b[0m terminals \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "# Transition Matrix as nested dict. \n",
    "# State -> Actions in state -> States by each action -> Probabilty\n",
    "\n",
    "t = {\n",
    "    \"A\": {\n",
    "            \"X\": {\"A\":0.3, \"B\":0.7},\n",
    "            \"Y\": {\"A\":1.0}            \n",
    "         },\n",
    "    \"B\": {\n",
    "            \"X\": TODO,            \n",
    "            \"Y\": {\"A\":1.0}\n",
    "         },\n",
    "    \"End\": {}\n",
    "}\n",
    "\n",
    "init = \"A\" # initial state\n",
    "\n",
    "terminals = [\"End\"]\n",
    "\n",
    "rewards = {\n",
    "    \"A\": 5,\n",
    "    \"B\": TODO,\n",
    "    \"End\": TODO  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition Matrix as nested dict. \n",
    "# State -> Actions in state -> States by each action -> Probabilty\n",
    "\n",
    "t = {\n",
    "    \"A\": {\n",
    "            \"X\": {\"A\":0.3, \"B\":0.7},\n",
    "            \"Y\": {\"A\":1.0}            \n",
    "         },\n",
    "    \"B\": {\n",
    "            \"X\": {\"End\":0.8, \"B\":0.2},            \n",
    "            \"Y\": {\"A\":1.0}\n",
    "         },\n",
    "    \"End\": {}\n",
    "}\n",
    "\n",
    "init = \"A\"\n",
    "\n",
    "terminals = [\"End\"]\n",
    "\n",
    "rewards = {\n",
    "    \"A\": 5,\n",
    "    \"B\": -10,\n",
    "    \"End\": 100   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.1.4 Creting the MDP module\n",
    "In order to define an MDP with the actual transition model defined above we will now create a custom class CustomMDP from Q6.1.1 (which calls the generic MDP class) internally and implement the required transition model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMDP(MDP):\n",
    "\n",
    "    def __init__(self, transition_matrix, rewards, terminals, init, gamma=.9):\n",
    "        # All possible actions.\n",
    "        actlist = []\n",
    "        for state in transition_matrix.keys():\n",
    "            actlist.extend(transition_matrix.keys())\n",
    "        actlist = list(set(actlist))\n",
    "\n",
    "        MDP.__init__(self, init, actlist, terminals=terminals, gamma=gamma)\n",
    "        self.t = transition_matrix\n",
    "        self.reward = rewards\n",
    "        for state in self.t:\n",
    "            self.states.add(state)\n",
    "\n",
    "    def T(self, state, action):\n",
    "        return [(new_state, prob) for new_state, prob in self.t[state][action].items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we instantiate the class with the parameters for our MDP in the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_mdp = CustomMDP(t, rewards, terminals, init, gamma=.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now probe the MDP in various ways including getting lists of the actions - or more interestingsly evaluate $P(S_{t+1}|S_{t}=A,Action=X)$ as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 0.3), ('B', 0.7)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_mdp.T('A','X')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font color=dark-magenta>TASK:</font> Does the outcome of `our_mdp.T('A','X')` match your expectation from the graph ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "`our_mdp.T('A','X')` should lead to: [('A', 0.3), ('B', 0.7)]\n",
    "\n",
    "<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>CHECKPOINT:</font> With this you should now know how to define and represent an MDP in Python.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.2 Grid MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q6.2-Q6.5 we'll look at a concrete implementation that makes use of the `MDP` from Q6.1 as a base class (similarly to the CustomMDP) to do some actual planning.\n",
    "\n",
    "We'll consider a grid world similar to **Fig 17.1** in the AIMA book (x,y are zero-indexed not one-indexed as in the book)\n",
    "\n",
    "\n",
    "<img src=\"./resources/maze_0c.png\" width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>CHECKPOINT:</font> Pay attention to the notation here !\n",
    "- $P_a$ refers to the case where the actual action taken is the intended action\n",
    "- $P_l$ refers to the case where the actual action taken is -90 degrees off the intended action, i.e. to the \"left\" of the intended action (not to the left in absolute terms except for when the intended action is up/north). \n",
    "- $P_r$ refers to the case where the actual action taken is +90 degrees off the intended action, i.e. to the \"right\" of the intended action (not literally to the right except for when the intended action is up/north)\n",
    "\n",
    "- For example; if the intended action is down (south) this is taken/achieved with probability $P_a$, but with probability $P_r$ the action is actually west i.e. \"right\" of the intended action. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.2.0 Why is this an interesting problem...?\n",
    "\n",
    "<br>\n",
    "\n",
    "- <font color=dark-magenta>TASK:</font> Give at least two \"real-world\" examples/scenarios where there is uncertainty related to the outcome of an action in a navigation scenario as the Grid world. Discuss with your fellow students or tutors to make sure you appreciate that this is not an uncommon scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "    \n",
    "There are many examples, e.g.: \n",
    "- Travel planning: When planning a multi-leg plane trip from Glasgow to Madrid (e.g. for a wedding); there are several uncertanties related to the outcome of each leg (e.g. delayes, detours and cancelleation). We normally account for these uncertanties without explicity thinking about it and trade off the risk of not geting to the wedding (i.e. not receiving the reward) and getting there (e.g. leaving early and choosing reliable airlines) \\\\\n",
    "\n",
    "- The paper: http://www.cs.uml.edu/ecg/uploads/AIfall14/MDPApplications3.pdf , provides a long list of applications where MDPs have been used to model a real-world problem.\n",
    "\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.2.1 Analysis of the GridMDP class\n",
    "\n",
    "The GridMDP class below is used to represent a grid world MDP like the one shown in **Fig 17.1** of the AIMA Book (with a few minor changes!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDP(MDP):\n",
    "\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state).  Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        Pa = 0.7\n",
    "        Pl = 0.1\n",
    "        Pr = 0.2 \n",
    "        \n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(Pa, self.go(state, action)),\n",
    "                    (Pr, self.go(state, turn_right(action))),\n",
    "                    (Pl, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"Return the state that results from going in this direction.\"\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {\n",
    "            (1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The **_ _init_ _** method takes **grid** as an extra parameter compared to the MDP class. The grid is a nested list of rewards in states.\n",
    "- **go** method returns the state by going in a particular direction by using vector_add.\n",
    "- **T** method is not implemented and is somewhat different from the text. Here we return (probability, s') pairs where s' belongs to a list of possible states by taking action a in state s.\n",
    "- **actions** method returns a list of actions possible in each state. By default, it returns all actions for states other than terminal states.\n",
    "- **to_arrows** are used for representing the policy in a grid-like format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=dark-magenta>TASK:</font> From inspecting the code for the GridMDP, determine the transition model (i.e. the uncertainty associated with the outcome of the actions). Write down the transition model as a table/list (you should exploit the Markov property implied by the problem definition). \n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "    \n",
    "The transition model is the same for all states\n",
    "\n",
    "If not runing into walls:\n",
    "- $P( \\text{Outcome-Is-North} | s, Action=Up/North) = P_a = 0.7$\n",
    "- $P( \\text{Outcome-Is-West}  | s, Action=Up/North) = P_l = 0.1$\n",
    "- $P( \\text{Outcome-Is-East}  | s, Action=Up/North) = P_r = 0.2$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $P( \\text{Outcome-Is-South} | s, Action=Down/South) = P_a = 0.7$\n",
    "- $P( \\text{Outcome-Is-East}  | s, Action=Down/South) = P_l = 0.1$\n",
    "- $P( \\text{Outcome-Is-West}  | s, Action=Down/South) = P_r = 0.2$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $P( \\text{Outcome-Is-West}  | s, Action=Left/West) = P_a = 0.7$\n",
    "- $P( \\text{Outcome-Is-South} | s, Action=Left/West) = P_l = 0.1$\n",
    "- $P( \\text{Outcome-Is-North} | s, Action=Left/West) = P_r = 0.2$\n",
    "\n",
    "<br>\n",
    "\n",
    "- $P( \\text{Outcome-Is-East}  | s, Action=Right/South) = P_a = 0.7$\n",
    "- $P( \\text{Outcome-Is-North} | s, Action=Right/South) = P_l = 0.1$\n",
    "- $P( \\text{Outcome-Is-South} | s, Action=Right/South) = P_r = 0.2$\n",
    "\n",
    "BUT if an action results in the agent hitting a wall then the agent stays in s with:\n",
    "\n",
    "$$P(\\text{Outcome-Is-Stay-In-s} \\,|\\, s, \\text{Action=Action-Leading-To-Hitting-Wall})$$\n",
    "\n",
    "e.g. if taking action Up/North in (0,0) there is 0.1 probability of ending up in (0,0) again.\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=dark-magenta>TASK:</font> The model is different from the transition model in the lecture/AIMA (Ch 17) - but how ? This could potentially change the policy but it is difficult to know exactaly before solving the Bellman equations... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "- The transition model has different probabilities, allthough the overall principle is the same.\n",
    "\n",
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.2.2 Create an instance of the GridMDP class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a GridMDP like the one in **Fig 17.1** as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rstep = -0.04  # the reward for being in a non-terminal state\n",
    "Rt    = 1      # defines the reward for ending in a terminal state\n",
    "gamma = 0.9    # discounting factor\n",
    "\n",
    "grid_world_4x3 = GridMDP([[Rstep, Rstep, Rstep, Rt],\n",
    "                          [Rstep, None,  Rstep, -Rt],\n",
    "                          [Rstep, Rstep, Rstep, Rstep]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* <font color=dark-magenta>TASK:</font> Gamma defines the discount factor i.e. how much you favor immediate rewards over future (potentially uncertain) rewards. Try to change it to 1 - what do you see and is this expected (hint: it might be worth looking up the role of gamma in the book)? (p.s. change it back to 0.9 and rerun the cell before continuing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "- $\\gamma$ indicates preference for immediate rewards over future (potentially) less certain rewards and should be between [0,1], however most nummerical algorithms requres gamma to be ]0,1[ for them to converge.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.3 Solving the GridMDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the actual goal is to obtain an optimal policy $\\pi$ i.e. find a function that maps from a state to a recommended action for all states which on average gives us the highest sum of discounted rewards. \n",
    "\n",
    "We'll do this in two steps:\n",
    "\n",
    "* 1) Assign utility (or value) to all states (Q6.3.0-Q6.3.3)\n",
    "    * The utility (or value) of each state is the expected sum of discounted future rewards.\n",
    "\n",
    "* 2) Using the maximum expected utility principle to select the best policy (Q6.3.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.3.0 Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Bellman (optimallity) equations relates the optimal utility of a state to the optimal utility of its neighboring states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK:</font> Inspect the lecture notes / book and write down the Bellman equations: a) in general and b) for state (0,0) and (1,0) in this particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "$${U }\\left( s \\right) = R\\left( s \\right) + \\gamma \\text{max}_a \\sum\\limits_{s'}^{} {p\\left( {s'|s,a } \\right)U\\left( {s'} \\right)} $$\n",
    "\n",
    "See also the lecture notes for U(0,0); make sure to account for all possible outcomes in each state before taking the max\n",
    "\n",
    "We will structure the elements inside the max operation as:\n",
    "\n",
    "$$\\begin{gathered}\n",
    "  {P_{\\text{a}}} \\times U({\\text{s' if}}\\,{\\text{the actual outcome is the intended action a}}) + {P_{\\text{l}}} \\times U({\\text{s' if}}\\,{\\text{the actual outcome is -90 degrees, i.e. to the \"left\" of the intended action}}) + {P_{\\text{r}}} \\times U({\\text{s' if}}\\,{\\text{the actual outcome is +90 degrees, i.e. to the \"right\" of the intended action}}) \\hfill \\\\\n",
    "   \\hfill \\\\ \n",
    "\\end{gathered}$$\n",
    "\n",
    "\n",
    "For state (0,0) (note: we use curly brackets just for convenience):\n",
    "$$\\begin{gathered}\n",
    "  s = \\{ 0,0\\}  \\hfill \\\\\n",
    "  U(s) = R(s) + \\gamma  \\times {\\max _a}\\{  \\hfill \\\\\n",
    "  P(\\{ 0,1\\} |up,s)U(\\{ 0,1\\} ) + P(\\{ 0,0\\} |up,s)U(\\{ 0,0\\} ) + P(\\{ 1,0\\} |up,s)U(\\{ 1,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(up) \\hfill \\\\\n",
    "  P(\\{ 0,0\\} |down,s)U(\\{ 0,0\\} ) + P(\\{ 1,0\\} |down,s)U(\\{ 1,0\\} ) + P(\\{ 0,0\\} |down,s)U(\\{ 0,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,(down) \\hfill \\\\\n",
    "  P(\\{ 0,0\\} |left,s)U(\\{ 0,0\\} ) + P(\\{ 0,0\\} |left,s)U(\\{ 0,0\\} ) + P(\\{ 0,1\\} |left,s)U(\\{ 0,1\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(left) \\hfill \\\\\n",
    "  P(\\{ 1,0\\} |right,s)U(\\{ 1,0\\} ) + P(\\{ 0,1\\} |right,s)U(\\{ 0,1\\} ) + P(\\{ 0,0\\} |right,s)U(\\{ 0,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(right) \\hfill \\\\\n",
    "  \\}  \\hfill \\\\\n",
    "   = R(s) + \\gamma  \\times {\\max _a}\\{  \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 0,1\\} ) + {P_{\\text{l}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 1,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(up) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 0,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(down) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 0,1\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(left) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 0,1\\} ) + {P_{\\text{r}}} \\times U(\\{ 0,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(right) \\hfill \\\\\n",
    "  \\}  \\hfill \\\\ \n",
    "\\end{gathered},$$\n",
    "where ${P_{\\text{a}}} = 0.7,{P_{\\text{l}}} = 0.1\\,and\\,{P_{\\text{r}}} = 0.2$.\n",
    "\n",
    "<br>\n",
    "\n",
    "For state (1,0):\n",
    "$$\\begin{gathered}\n",
    "  s = \\{ 1,0\\}  \\hfill \\\\\n",
    "  U(s) = R(s) + \\gamma  \\times {\\max _a}\\{  \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 2,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,(up = north) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 2,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 0,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,(down = south) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 0,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 1,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,(left = west) \\hfill \\\\\n",
    "  {P_{\\text{a}}} \\times U(\\{ 2,0\\} ) + {P_{\\text{l}}} \\times U(\\{ 1,0\\} ) + {P_{\\text{r}}} \\times U(\\{ 1,0\\} ),\\,\\,\\,\\,\\,\\,\\,\\,\\,\\,(right = east) \\hfill \\\\\n",
    "  \\}  \\hfill \\\\ \n",
    "\\end{gathered} $$\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK:</font> Explain (to the tutors or formulate in writing) why it is difficult to solve the Bellman equations for U(s) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "- The Bellman (optimiallay) equations is a set of non-linear equations which requires nummerical solutions as no closed-form/analytical solutions exists.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>CHECKPOINT</font>: At this point it should be clear to you that __manually__ evaluating and solving the Bellman equations is a difficult and tedious task - and that we need nummerical algorithms to help us out...</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.3.1 Solving the Bellman equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have looked at two methods for solving the Bellman equations, namely Value Iteration and Policy Iteration. Let's first have a look at code for value iteration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a partial implementation of the Value Iteration algorithm - although with a few instructions missing (look for \"TODO\"!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1500979268.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    if delta < TODO\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(mdp, epsilon=0.0001):\n",
    "    \"Solving an MDP by value iteration. [Figure 17.4]\"\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if delta < TODO\n",
    "            return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes as inputs two parameters: an MDP to solve and epsilon the maximum error allowed in the utility of any state. It returns a dictionary containing utilities where the keys are the states and values represent utilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK:</font> Inspect the code and validate that the code implements the Bellman equations. You may want to insert a few \"print\" statements to inspect the behavior of value itereation (e.g. for delta and epsilon)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK:</font> The code is missing an implementation of the convergence criterion based on the maxium change in utilities from iteration to iteration. Define a suitable criterion (look in the book / lecture notes) and replace \"TODO\" with a suitable expression ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp, epsilon, verbose=False):\n",
    "    \"Solving an MDP by value iteration. [Figure 17.4]\"\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    if verbose:\n",
    "        print(\"epsilon=\" + str(epsilon))    \n",
    "        print(\"epsilon * (1 - gamma) / gamma=\" + str(epsilon * (1 - gamma) / gamma))    \n",
    "        print(\"\")\n",
    "        \n",
    "    while True:\n",
    "        U = U1.copy()\n",
    "        delta = 0\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "            delta = max(delta, abs(U1[s] - U[s]))\n",
    "        if verbose:\n",
    "            print(\"delta =\" + str(delta))\n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "            return U\n",
    "        \n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.3.2 Running Value Iteration (Grid World)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run value iteration and assuming you have implemented a correct convergence procedure we should get a set of optimal utilities by running the follwing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=1e-05\n",
      "epsilon * (1 - gamma) / gamma=1.111111111111111e-06\n",
      "\n",
      "delta =1.0\n",
      "delta =0.6192\n",
      "delta =0.380376\n",
      "delta =0.23088888\n",
      "delta =0.1375867944\n",
      "delta =0.10000963792799994\n",
      "delta =0.07793277572159996\n",
      "delta =0.0579059950260096\n",
      "delta =0.03627192152273437\n",
      "delta =0.021940308935482322\n",
      "delta =0.017675336517464876\n",
      "delta =0.013862138764932593\n",
      "delta =0.010086217647903822\n",
      "delta =0.006866448046995238\n",
      "delta =0.004528081434064718\n",
      "delta =0.0029314933371660246\n",
      "delta =0.0021645147514174345\n",
      "delta =0.0018063696367128512\n",
      "delta =0.001497827992839637\n",
      "delta =0.0012358135483882399\n",
      "delta =0.0010157492425797027\n",
      "delta =0.0008324510914428651\n",
      "delta =0.0006807285921324874\n",
      "delta =0.0005557280230736039\n",
      "delta =0.0004531015804034172\n",
      "delta =0.0003690637103440225\n",
      "delta =0.0003003819288751042\n",
      "delta =0.0002443332444631574\n",
      "delta =0.0001986457632392885\n",
      "delta =0.0001614368865194421\n",
      "delta =0.00013115426122120577\n",
      "delta =0.00010652241099226578\n",
      "delta =8.649609722533036e-05\n",
      "delta =7.022043129187305e-05\n",
      "delta =5.6997246403234225e-05\n",
      "delta =4.625702212467109e-05\n",
      "delta =3.7535602874311236e-05\n",
      "delta =3.0454983962854842e-05\n",
      "delta =2.4707509114707538e-05\n",
      "delta =2.0042906755640688e-05\n",
      "delta =1.6257675541170302e-05\n",
      "delta =1.3186406383827731e-05\n",
      "delta =1.0694696147534044e-05\n",
      "delta =8.673366724068488e-06\n",
      "delta =7.0337528751684975e-06\n",
      "delta =5.7038639158096816e-06\n",
      "delta =4.6252590586853826e-06\n",
      "delta =3.7505050488051417e-06\n",
      "delta =3.0411085139153693e-06\n",
      "delta =2.465835058015897e-06\n",
      "delta =1.9993432355119856e-06\n",
      "delta =1.6210747589284757e-06\n",
      "delta =1.3143531185089152e-06\n",
      "delta =1.0656516471058142e-06\n",
      "{(0, 1): 0.2840738544194152, (1, 2): 0.5494998808119402, (2, 1): 0.2621950733195458, (0, 0): 0.17107531816691787, (3, 1): -1.0, (2, 0): 0.1219328593434591, (3, 0): -0.06447553917794453, (0, 2): 0.39265724259620166, (2, 2): 0.7002143122767938, (1, 0): 0.09284552881882457, (3, 2): 1.0}\n"
     ]
    }
   ],
   "source": [
    "U = value_iteration(grid_world_4x3,epsilon=0.00001,verbose=True)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Green>CHECKPOINT</font>If you have implemented everything and not changed any parameters (e.g. gamma=0.9) you should get that U(1, 2) is approx. 0.55. and (0, 2): 0.39."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magenta>TASK:</font> Now, try changing epsilon which should influence the precision of the solution as the decision to stop iterating depends on this parameter. Try for example to set epsilon=0.5 and rerun value_iteration and check if the solution is now different that the (precise solution we obtained before?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT YOUR CODE WITH EPSILON=0.5 HERE!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=0.5\n",
      "epsilon * (1 - gamma) / gamma=0.05555555555555554\n",
      "\n",
      "delta =1.0\n",
      "delta =0.6192\n",
      "delta =0.380376\n",
      "delta =0.23088888\n",
      "delta =0.1375867944\n",
      "delta =0.10000963792799994\n",
      "delta =0.07793277572159996\n",
      "delta =0.0579059950260096\n",
      "delta =0.03627192152273437\n",
      "{(0, 1): 0.2186840540899471, (1, 2): 0.5422257504880518, (2, 1): 0.24082641168329108, (0, 0): 0.06923588077160955, (3, 1): -1.0, (2, 0): 0.0864342412610791, (3, 0): -0.11328454530441764, (0, 2): 0.3601564407850999, (2, 2): 0.6949601032175599, (1, 0): 0.008364549712402282, (3, 2): 1.0}\n"
     ]
    }
   ],
   "source": [
    "U = value_iteration(grid_world_4x3,epsilon=0.5, verbose=True)\n",
    "print(U)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q6.3.3 Visualization for Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q6.3.3a) Saving the utilities for each iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate how value iteration conconverge and that \"utility\" propagate out of certain states let us create a simple visualisation. We will be using a modified version of the value_iteration function which will store U for each iteration. We will also remove the parameter epsilon and instead add the number of iterations we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration_instru(mdp, iterations=20):\n",
    "    U_over_time = []\n",
    "    U1 = {s: 0 for s in mdp.states}\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for _ in range(iterations):\n",
    "        U = U1.copy()\n",
    "        for s in mdp.states:\n",
    "            U1[s] = R(s) + gamma * max([sum([p * U[s1] for (p, s1) in T(s, a)])\n",
    "                                        for a in mdp.actions(s)])\n",
    "        U_over_time.append(U)\n",
    "    return U_over_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=green>CHECKPOINT:</font>Make sure you understand how the algorithms terminates, i.e. we are now stopping after a certain number iterations not informed by the change in the utilities.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now run value_iteration_instru for 100 iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_iterations = value_iteration_instru(grid_world_4x3,100)           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magent>TASK:</font> Write code to inspect the values at timestep t=0, t=1, and the final iteration. You should notice how the values change from iteration to iteration and that we eventually end up with similar values as the initial value_iteration algorithm (with the epsilon option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code to inspect U_iterations here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 1): 0, (1, 2): 0, (2, 1): 0, (0, 0): 0, (3, 1): 0, (2, 0): 0, (3, 0): 0, (0, 2): 0, (2, 2): 0, (1, 0): 0, (3, 2): 0}\n",
      "{(0, 1): -0.04, (1, 2): -0.04, (2, 1): -0.04, (0, 0): -0.04, (3, 1): -1.0, (2, 0): -0.04, (3, 0): -0.04, (0, 2): -0.04, (2, 2): -0.04, (1, 0): -0.04, (3, 2): 1.0}\n",
      "{(0, 1): 0.284074641734198, (1, 2): 0.5495004286854264, (2, 1): 0.26219695577943714, (0, 0): 0.17107628920398532, (3, 1): -1.0, (2, 0): 0.12193569799185791, (3, 0): -0.06446990867840287, (0, 2): 0.39265791821512636, (2, 2): 0.7002147824555868, (1, 0): 0.09284666052708257, (3, 2): 1.0}\n"
     ]
    }
   ],
   "source": [
    "print(U_iterations[0])\n",
    "\n",
    "print(U_iterations[1])\n",
    "\n",
    "print(U_iterations[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=dark-magent>TASK:</font> Make a graph (e.g. using matlibplot) of the utility values as a function of the of the iteration number for each state showing the convergence of the algorithm by plotting the values in `U_iterations`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_utility_estimates(U,mdp, no_of_iterations, states_to_plot):\n",
    "    graphs = {state:[] for state in states_to_plot}\n",
    "    \n",
    "    for iteration in range(1,no_of_iterations+1):\n",
    "        for state, value in graphs.items():        \n",
    "            graphs[state].append((iteration, U_iterations[iteration][state]))\n",
    "    \n",
    "    for state, value in graphs.items():\n",
    "        state_x, state_y = zip(*value)\n",
    "        plt.plot(state_x, state_y, label=str(state))\n",
    "    \n",
    "    plt.ylim([-2.1,2.1])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('U est')\n",
    "    plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_utility_estimates(U_iterations, grid_world_4x3 , 20, [(0,0),(0,1),(0,2),(1,0),(1,2),(2,0),(2,1),(2,2),(3,0),(3,1),(3,2)])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: We note that the algorithm has converged to a solution within 15 iterations in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Q6.3.3b [**OPTIONAL**]: Widgets  <font color=red> (you can skip this step if your pywidgets installation is not working!)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function to create the visualisation from the utilities returned by **value_iteration_instru**. The reader need not concern herself/himself with the code that immediately follows as it is the usage of Matplotib with IPython Widgets. If you are interested in reading more about these visit [ipywidgets.readthedocs.io](http://ipywidgets.readthedocs.io)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plot_grid_step_function(columns, row, U_over_time):\n",
    "    '''ipywidgets interactive function supports\n",
    "       single parameter as input. This function\n",
    "       creates and return such a function by taking\n",
    "       in input other parameters\n",
    "    '''\n",
    "    def plot_grid_step(iteration):\n",
    "        data = U_over_time[iteration]\n",
    "        data = defaultdict(lambda: 0, data)\n",
    "        grid = []\n",
    "        for row in range(rows):\n",
    "            current_row = []\n",
    "            for column in range(columns):\n",
    "                current_row.append(data[(column, row)])\n",
    "            grid.append(current_row)\n",
    "        grid.reverse() # output like book\n",
    "        fig = plt.imshow(grid, cmap=plt.cm.bwr, interpolation='nearest')\n",
    "\n",
    "        plt.axis('off')\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "        for col in range(len(grid)):\n",
    "            for row in range(len(grid[0])):\n",
    "                magic = grid[col][row]\n",
    "                fig.axes.text(row, col, \"{0:.2f}\".format(magic), va='center', ha='center')\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    return plot_grid_step\n",
    "\n",
    "def make_visualize(slider):\n",
    "    ''' Takes an input a slider and returns \n",
    "        callback function for timer and animation\n",
    "    '''\n",
    "    \n",
    "    def visualize_callback(Visualize, time_step):\n",
    "        if Visualize is True:\n",
    "            for i in range(slider.min, slider.max + 1):\n",
    "                slider.value = i\n",
    "                time.sleep(float(time_step))\n",
    "    \n",
    "    return visualize_callback\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = 4\n",
    "rows = 3\n",
    "plot_grid_step = make_plot_grid_step_function(columns, rows, U_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the follwing code and you should see the values of the utilities in each iteration (depending on your widget installation you may also see some sliders):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0fbc15dae4245278538e72abee245e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='iteration', max=25, min=1), Output()), _dom_classes=('wiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d948e4790c049df8f92e541b9b7df84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(ToggleButton(value=True, description='Visualize'), ToggleButtons(description='Extra Delaâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration_slider = widgets.IntSlider(min=1, max=25, step=1, value=0)\n",
    "w=widgets.interactive(plot_grid_step,iteration=iteration_slider)\n",
    "display(w)\n",
    "\n",
    "visualize_callback = make_visualize(iteration_slider)\n",
    "\n",
    "visualize_button = widgets.ToggleButton(description = \"Visualize\", value = True)\n",
    "#time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0', '0.1', '0.2', '0.5', '0.7', '1.0'])\n",
    "time_select = widgets.ToggleButtons(description='Extra Delay:',options=['0.5'])\n",
    "a = widgets.interactive(visualize_callback, Visualize = visualize_button, time_step=time_select)\n",
    "\n",
    "display(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color=dark-magent>TASK:</FONT>Inspect the plot above to observe how the utility changes across iterations. If (and only if) your widgets installation is correct, it is also possible to move the slider using arrow keys or to jump to the value by directly editing the number with a double click. The **Visualize Button** will automatically animate the slider for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6.3.4 Finding the policy: Maximum Expected Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the estimated utility for each state, we can now determine the best policy by selecting the action with the maximum expected utility.\n",
    "\n",
    "Let us first define the required functions by running the follwing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_utility(a, s, U, mdp):\n",
    "    \"The expected utility of doing a in state s, according to the MDP and U.\"\n",
    "    return sum([p * U[s1] for (p, s1) in mdp.T(s, a)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_policy(mdp, U):\n",
    "    \"\"\"Given an MDP and a utility function U, determine the best policy,\n",
    "    as a mapping from state to action. (Equation 17.4)\"\"\"\n",
    "    pi = {}\n",
    "    for s in mdp.states:\n",
    "        pi[s] = argmax(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "    return pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, let's find the recommend policy by calling the relevant functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 1): (0, 1),\n",
       " (1, 2): (1, 0),\n",
       " (2, 1): (-1, 0),\n",
       " (0, 0): (0, 1),\n",
       " (3, 1): None,\n",
       " (2, 0): (0, 1),\n",
       " (3, 0): (0, -1),\n",
       " (0, 2): (1, 0),\n",
       " (2, 2): (1, 0),\n",
       " (1, 0): (-1, 0),\n",
       " (3, 2): None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_est = value_iteration(grid_world_4x3, 0.00001) # rerun solver\n",
    "pi_opt = best_policy(grid_world_4x3, U_est)\n",
    "pi_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the solution in terms of recommended actions in each state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">   >      >   .\n",
      "^   None   <   .\n",
      "^   <      ^   v\n"
     ]
    }
   ],
   "source": [
    "print_table(grid_world_4x3.to_arrows(pi_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* <font color=dark-magent>TASK:</font>Examine the solution and discuss if/how this makes sense in terms of the transitions model and rewards (hint: start by explaning why the agent policy suggests that we should go left in state (1,0) (note: the python grid maze is zero-indexed) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\">\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "It makes sense given that the cost of making a step in the world (-0.04) is low compared to the potential gain (+1), e.g. in (1,0) we go left (i.e. a detour) to avoid the potentially negative reward in (3,1). This will change depending on the rewards and transition model (see Q6.4).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.4 Changing the MDP: State-space, transition models, reward structure and discounting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark-magent>KEY TASK:</font>\n",
    "We suggest that you experiment with value-iteration and policy finding methods to understand how the policy change has a result of different models, parameters settings. To do this we suggest that you iterate the following steps:\n",
    "\n",
    "Modify the transition model, rerun the solver and inspect the policy for the following cases (i.e. rerun the code in Q6.4.1):\n",
    "* a) Modify the transition mode so Pr is now 0.1 and Pl is 0.2; does that change the policy ?\n",
    "* b) Set the transition model to be very sure or very insecure about outcome of actions (e.g. 98% sure of making the intended action)\n",
    "* c) Set the discount factor to 0.001 ($\\approx 0$) (what does this mean?)\n",
    "* d) Change the size of the rewards (set e.g. Rstep to -2 and observe that the agent simply wants to leave the environment as soon as possible) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define a version of the Grid world that allows us to easily change the transition model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridMDPV2(MDP):\n",
    "\n",
    "    \"\"\"A two-dimensional grid MDP, as in [Figure 17.1].  All you have to do is\n",
    "    specify the grid as a list of lists of rewards; use None for an obstacle\n",
    "    (unreachable state).  Also, you should specify the terminal states.\n",
    "    An action is an (x, y) unit vector; e.g. (1, 0) means move east.\"\"\"\n",
    "\n",
    "    def __init__(self, grid, terminals, init=(0, 0), gamma=.9,Pa=0.7,Pl=.1, Pr=0.2):\n",
    "        grid.reverse()  # because we want row 0 on bottom, not on top\n",
    "        MDP.__init__(self, init, actlist=orientations,\n",
    "                     terminals=terminals, gamma=gamma)\n",
    "        self.grid = grid\n",
    "        self.rows = len(grid)\n",
    "        self.cols = len(grid[0])\n",
    "        self.Pa = Pa\n",
    "        self.Pl = Pl\n",
    "        self.Pr = Pr\n",
    "        for x in range(self.cols):\n",
    "            for y in range(self.rows):\n",
    "                self.reward[x, y] = grid[y][x]\n",
    "                if grid[y][x] is not None:\n",
    "                    self.states.add((x, y))\n",
    "\n",
    "    def T(self, state, action):\n",
    "        if action is None:\n",
    "            return [(0.0, state)]\n",
    "        else:\n",
    "            return [(self.Pa, self.go(state, action)),\n",
    "                    (self.Pr, self.go(state, turn_right(action))),\n",
    "                    (self.Pl, self.go(state, turn_left(action)))]\n",
    "\n",
    "    def go(self, state, direction):\n",
    "        \"Return the state that results from going in this direction.\"\n",
    "        state1 = vector_add(state, direction)\n",
    "        return state1 if state1 in self.states else state\n",
    "\n",
    "    def to_grid(self, mapping):\n",
    "        \"\"\"Convert a mapping from (x, y) to v into a [[..., v, ...]] grid.\"\"\"\n",
    "        return list(reversed([[mapping.get((x, y), None)\n",
    "                               for x in range(self.cols)]\n",
    "                              for y in range(self.rows)]))\n",
    "\n",
    "    def to_arrows(self, policy):\n",
    "        chars = {\n",
    "            (1, 0): '>', (0, 1): '^', (-1, 0): '<', (0, -1): 'v', None: '.'}\n",
    "        return self.to_grid({s: chars[a] for (s, a) in policy.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>Q6.4a: Modify the transition mode so Pright is now 0.1 and Pleft is 0.2; does that change the policy ?</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Policy:\n",
      "{(0, 1): 0.33750442056754393, (1, 2): 0.6069380874085165, (2, 1): 0.43056683408866203, (0, 0): 0.22980546137290622, (3, 1): -1.0, (2, 0): 0.26708227146879165, (3, 0): 0.0466601660350543, (0, 2): 0.45456874074936526, (2, 2): 0.7667695304611012, (1, 0): 0.17570024507918186, (3, 2): 1.0}\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      "^   >      ^   <\n"
     ]
    }
   ],
   "source": [
    "Rstep = -0.04  # the reward for being in a non-terminal state\n",
    "Rt    = 1      # defines the reward for ending in a terminal state\n",
    "gamma = 0.9 # discounting factor\n",
    "\n",
    "grid_world_4x3_a = GridMDPV2([[Rstep, Rstep, Rstep, Rt],\n",
    "                          [Rstep, None,  Rstep, -Rt],\n",
    "                          [Rstep, Rstep, Rstep, Rstep]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma,\n",
    "                          #Pa=0.7,Pl=.1, Pr=0.2  #this is the orginal transition model\n",
    "                          Pa=0.7,Pl=.2, Pr=0.1  \n",
    "                          )\n",
    "print(\"\")\n",
    "print(\"Policy:\")\n",
    "U_est_a = value_iteration(grid_world_4x3_a, 0.00001) # rerun solver\n",
    "print(U_est_a)\n",
    "\n",
    "pi_opt_a = best_policy(grid_world_4x3_a, U_est_a)\n",
    "print_table(grid_world_4x3_a.to_arrows(pi_opt_a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: You should see that the action in (1,0) has changed because intuatively there is now little chance (albeit) not zero chance of ending up in the the bad state (-1 reward) if in (2,1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>Q6.4b: Set the transition model to be very sure or very insecure about the outcome of actions (e.g. 98% sure of making the intended action)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities:\n",
      "{(0, 1): 0.5185399999999999, (1, 2): 0.734, (2, 1): 0.734, (0, 0): 0.42668599999999995, (3, 1): -1.0, (2, 0): 0.6205999999999999, (3, 0): 0.5185399999999999, (0, 2): 0.6205999999999999, (2, 2): 0.86, (1, 0): 0.5185399999999999, (3, 2): 1.0}\n",
      "\n",
      "Policy:\n",
      ">   >      >   .\n",
      "^   None   ^   .\n",
      ">   >      ^   <\n"
     ]
    }
   ],
   "source": [
    "Rstep = -0.04  # the reward for being in a non-terminal state\n",
    "Rt    = 1      # defines the reward for ending in a terminal state\n",
    "gamma = 0.9 # discounting factor\n",
    "\n",
    "grid_world_4x3_b = GridMDPV2([[Rstep, Rstep, Rstep, Rt],\n",
    "                          [Rstep, None,  Rstep, -Rt],\n",
    "                          [Rstep, Rstep, Rstep, Rstep]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma,\n",
    "                          #Pa=0.7,Pl=.1, Pr=0.2  #this is the orginal transition model\n",
    "                          #Pa=0.99,Pl=.005, Pr=.005  \n",
    "                          Pa=1,Pl=0.0, Pr=0.0  \n",
    "                          )\n",
    "U_est_b = value_iteration(grid_world_4x3_b, 0.00001) # rerun solver\n",
    "pi_opt_b = best_policy(grid_world_4x3_b, U_est_b)\n",
    "\n",
    "print(\"Utilities:\")\n",
    "print(U_est_b)\n",
    "print(\"\")\n",
    "print(\"Policy:\")\n",
    "print_table(grid_world_4x3_b.to_arrows(pi_opt_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: Being very sure does not change the policy from Q6.4b allthough notice that the utilities are different (i.e. the policy can be the same eventhough the utilities are different!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>... let's try out a very insecure agent:</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities:\n",
      "{(0, 1): -0.026974662435346936, (1, 2): 0.2631575210871137, (2, 1): 0.12377251267673803, (0, 0): -0.12023184591961888, (3, 1): -1.0, (2, 0): -0.06207309124242295, (3, 0): -0.14655721711678696, (0, 2): 0.0973678164747648, (2, 2): 0.4842102953875574, (1, 0): -0.1352743754198828, (3, 2): 1.0}\n",
      "\n",
      "Policy:\n",
      "^   >      ^   .\n",
      "^   None   <   .\n",
      "<   v      <   v\n"
     ]
    }
   ],
   "source": [
    "Rstep = -0.04  # the reward for being in a non-terminal state\n",
    "Rt    = 1      # defines the reward for ending in a terminal state\n",
    "gamma = 0.9 # discounting factor\n",
    "\n",
    "grid_world_4x3_bb = GridMDPV2([[Rstep, Rstep, Rstep, Rt],\n",
    "                          [Rstep, None,  Rstep, -Rt],\n",
    "                          [Rstep, Rstep, Rstep, Rstep]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma,\n",
    "                          #Pa=0.7,Pl=.1, Pr=0.2  #this is the orginal transition model\n",
    "                          #Pa=0.99,Pl=.005, Pr=.005  \n",
    "                          Pa=1.0/3.0,Pl=1.0/3.0, Pr=1.0/3.0  \n",
    "                          )\n",
    "U_est_bb = value_iteration(grid_world_4x3_bb, 0.00001) # rerun solver\n",
    "pi_opt_bb = best_policy(grid_world_4x3_bb, U_est_bb)\n",
    "\n",
    "print(\"Utilities:\")\n",
    "print(U_est_bb)\n",
    "print(\"\")\n",
    "print(\"Policy:\")\n",
    "print_table(grid_world_4x3_bb.to_arrows(pi_opt_bb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: Now we want to avoid the -1 reward state because there is a high risk of ending up there if we get near it. In (2,3) we even suggest to move up rather than try to go directly for (3,2) because there is equal chance of ending up in (1,3),(2,3),(3,2) and up but the up action is not associated with any chance of going down (and ending up in the -1 state). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>Q6.4c) Set the discount factor to 0 (what does this mean?)</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon=1e-05\n",
      "epsilon * (1 - gamma) / gamma=0.00999\n",
      "\n",
      "delta =1.0\n",
      "delta =0.0006880000000000011\n",
      "Utilities:\n",
      "{(0, 1): -0.04, (1, 2): -0.04, (2, 1): -0.04, (0, 0): -0.04, (3, 1): -1.0, (2, 0): -0.04, (3, 0): -0.04, (0, 2): -0.04, (2, 2): -0.04, (1, 0): -0.04, (3, 2): 1.0}\n",
      "\n",
      "Policy:\n",
      ">   >      >   .\n",
      ">   None   <   .\n",
      ">   >      >   v\n"
     ]
    }
   ],
   "source": [
    "Rstep = -0.04  # the reward for being in a non-terminal state\n",
    "Rt    = 1      # defines the reward for ending in a terminal state\n",
    "gamma_c = 0.001 # discounting factor\n",
    "\n",
    "grid_world_4x3_c = GridMDPV2([[Rstep, Rstep, Rstep, Rt],\n",
    "                          [Rstep, None,  Rstep, -Rt],\n",
    "                          [Rstep, Rstep, Rstep, Rstep]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma_c,\n",
    "                          Pa=0.7,Pl=.1, Pr=0.2  #this is the orginal transition model                    \n",
    "                          )\n",
    "U_est_c = value_iteration(grid_world_4x3_c, 0.00001, verbose=True) # rerun solver\n",
    "\n",
    "print(\"Utilities:\")\n",
    "print(U_est_c)\n",
    "print(\"\")\n",
    "print(\"Policy:\")\n",
    "\n",
    "pi_opt_c = best_policy(grid_world_4x3_c, U_est_c)\n",
    "print_table(grid_world_4x3_c.to_arrows(pi_opt_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: Setting $\\gamma$ very close to zero means that we favor immediate rewards over future (potential) reward. You should see that the utilities convege to roughly the rewards for the individual state beause $G_t\\approx R(S)$ not involving any future rewards (as is always the case for the temrinal states). Setting $\\gamma=0$ will often lead to non-convergence or even divide by zero issues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=Red>Q6.4d) Change the size of the rewards (set e.g. Rstep to -2 and observe that the agent simply wants to leave the environment as soon as possible) </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities:\n",
      "{(0, 1): -8.978228465961493, (1, 2): -4.701706046161707, (2, 1): -3.8822162985649324, (0, 0): -9.388321176804716, (3, 1): -1.0, (2, 0): -5.8200545084699815, (3, 0): -3.84610354281084, (0, 2): -7.228742369861085, (2, 2): -2.273405421203089, (1, 0): -7.762512785331908, (3, 2): 1.0}\n",
      "\n",
      "Policy:\n",
      ">   >      >   .\n",
      "^   None   >   .\n",
      ">   >      >   ^\n"
     ]
    }
   ],
   "source": [
    "Rstep_d = -2 # the reward for being in a non-terminal state\n",
    "Rt    = 1    # defines the reward for ending in a terminal state\n",
    "gamma = 0.9  # discounting factor\n",
    "\n",
    "grid_world_4x3_d = GridMDPV2([[Rstep_d, Rstep_d, Rstep_d, Rt],\n",
    "                          [Rstep_d, None,  Rstep_d, -Rt],\n",
    "                          [Rstep_d, Rstep_d, Rstep_d, Rstep_d]],\n",
    "                          terminals=[(3, 2), (3, 1)],\n",
    "                          init=[(0, 0)],\n",
    "                          gamma=gamma,\n",
    "                          Pa=0.7,Pl=.1, Pr=0.2  #this is the orginal transition model                         \n",
    "                          )\n",
    "U_est_d = value_iteration(grid_world_4x3_d, 0.00001, verbose=False) # rerun solver\n",
    "pi_opt_d = best_policy(grid_world_4x3_d, U_est_d)\n",
    "\n",
    "print(\"Utilities:\")\n",
    "print(U_est_d)\n",
    "print(\"\")\n",
    "print(\"Policy:\")\n",
    "print_table(grid_world_4x3_d.to_arrows(pi_opt_d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CHECKPOINT: The agent now want to leave i.e. aims directly for a terminal state regardless of the reward recieved at the terminal state. The cost of walking around is simply to great for the agent to do detours. It even suggest going right in state (0,0). Getting out asap is the rational choice in this case !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:2px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6.5 Policy Iteration [**OPTIONAL**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=dark-magent>TASK:</font> Solve the GridMDP problem above using the Policy Iteration method and compare the solution with Value Iteration (you should find the same solution)\n",
    "\n",
    "<font color=dark-magent>TASK:</font> Measure and compare the rate of convergence by plotting the convergence of the utiltities.\n",
    "\n",
    "\n",
    "* Hint: The functions below implement the policy iteration solver from AIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp):\n",
    "    \"Solve an MDP by policy iteration [Figure 17.7]\"\n",
    "    U = {s: 0 for s in mdp.states}\n",
    "    pi = {s: random.choice(mdp.actions(s)) for s in mdp.states}\n",
    "    while True:\n",
    "        U = policy_evaluation(pi, U, mdp)\n",
    "        unchanged = True\n",
    "        for s in mdp.states:\n",
    "            a = argmax(mdp.actions(s), key=lambda a: expected_utility(a, s, U, mdp))\n",
    "            if a != pi[s]:\n",
    "                pi[s] = a\n",
    "                unchanged = False\n",
    "        if unchanged:\n",
    "            return pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(pi, U, mdp, k=20):\n",
    "    \"\"\"Return an updated utility mapping U from each state in the MDP to its\n",
    "    utility, using an approximation (modified policy iteration).\"\"\"\n",
    "    R, T, gamma = mdp.R, mdp.T, mdp.gamma\n",
    "    for i in range(k):\n",
    "        for s in mdp.states:\n",
    "            U[s] = R(s) + gamma * sum([p * U[s1] for (p, s1) in T(s, pi[s])])\n",
    "    return U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "widgets": {
   "state": {
    "c77ee9aaf1034625bff0f4ea499785ea": {
     "views": [
      {
       "cell_index": 69
      }
     ]
    },
    "fdb2d89e608445a2a7183abf4dc5b385": {
     "views": [
      {
       "cell_index": 69
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
