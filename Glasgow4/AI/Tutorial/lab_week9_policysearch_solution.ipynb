{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence - COMPSCI4004 2025-2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 9: Policy search (including function approximations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9.1 Introduction & Housekeeping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab is a case study where we attempt to apply (\"tabular\") policy search (and simple function approximations) to a specific problem namely an Open AI Gym compatible problem called CliffWalk.\n",
    "\n",
    "Running the notebook:\n",
    " - **We recommned you to run this lab on Google colab: https://colab.research.google.com/**\n",
    " - You will need to use an OLDER version of gym (0.21.0). If you are using Anaconda, you may run the following command in the console to change the gym version:\n",
    "     \n",
    "\n",
    "*conda install -c conda-forge gym==0.21.0*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's import everything we need for this lab (assuming you have installed Open AI gym and Tensorflow correctly). Note: you might get some FutureWarnings when importing tensorflow which is expected/typical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: setuptools==65.5.0 in c:\\users\\edmon\\anaconda3\\lib\\site-packages (65.5.0)\n",
      "Requirement already satisfied: wheel==0.38.4 in c:\\users\\edmon\\anaconda3\\lib\\site-packages (0.38.4)\n",
      "Requirement already satisfied: gym==0.21.0 in c:\\users\\edmon\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\edmon\\anaconda3\\lib\\site-packages (from gym==0.21.0) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.0 in c:\\users\\edmon\\anaconda3\\lib\\site-packages (from gym==0.21.0) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "# You can install v1.15 tensorflow but we recommend running the notebook on colab as its current default is 1.x\n",
    "# Be careful with python versions, keras and tensorflow!\n",
    "#!pip install q tensorflow==2.2.4\n",
    "#!pip install q keras==1.15\n",
    "\n",
    "# The following 3 statements are un-commented for runningon Google colab\n",
    "!pip install setuptools==65.5.0\n",
    "!pip install wheel==0.38.4\n",
    "!pip install gym==0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "import matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "# Import the open AI gym\n",
    "import gym\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "#from keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from keras import backend as K\n",
    "\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import utils as np_utils\n",
    "from keras import optimizers\n",
    "from keras import initializers\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "print(sys.version)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a few helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a couple of helper functions\n",
    "\n",
    "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if noshow:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if noshow:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9.2 The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start out with a relatively simple environment to demonstrate the policy search procedure. You can (optionally) choose to experiment with a more advanced/interesting example in a later task).\n",
    "\n",
    "The task is to transverse a grid world from start (3,0) to the target state ('T') where there is the risk of falling off a cliff ('C') with reward -100. All other states have a reward of -1.\n",
    "\n",
    "Let's define and load the environment and visualize it (see below):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "\n",
    "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
    "\n",
    "    metadata = {'render.modes': ['human', 'ansi']}\n",
    "\n",
    "    def _limit_coordinates(self, coord):\n",
    "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
    "        coord[0] = max(coord[0], 0)\n",
    "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
    "        coord[1] = max(coord[1], 0)\n",
    "        return coord\n",
    "\n",
    "    def _calculate_transition_prob(self, current, delta):\n",
    "        new_position = np.array(current) + np.array(delta)\n",
    "        new_position = self._limit_coordinates(new_position).astype(int)\n",
    "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
    "        reward = -100.0 if self._cliff[tuple(new_position)] else -1.0\n",
    "        is_done = self._cliff[tuple(new_position)] or (tuple(new_position) == (3,11))\n",
    "        return [(1.0, new_state, reward, is_done)]\n",
    "\n",
    "    def __init__(self):\n",
    "        self.shape = (4, 12)\n",
    "\n",
    "        nS = np.prod(self.shape)\n",
    "        nA = 4\n",
    "\n",
    "        # Cliff Location\n",
    "        self._cliff = np.zeros(self.shape, dtype=bool)\n",
    "        self._cliff[3, 1:-1] = True\n",
    "\n",
    "        # Calculate transition probabilities\n",
    "        P = {}\n",
    "        for s in range(nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            P[s] = { a : [] for a in range(nA) }\n",
    "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
    "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
    "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
    "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
    "\n",
    "        # We always start in state (3, 0)\n",
    "        isd = np.zeros(nS)\n",
    "        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\n",
    "\n",
    "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self._render(mode, close)\n",
    "\n",
    "    def _render(self, mode='human', close=False):\n",
    "        if close:\n",
    "            return\n",
    "\n",
    "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            position = np.unravel_index(s, self.shape)\n",
    "            # print(self.s)\n",
    "            if self.s == s:\n",
    "                output = \" x \"\n",
    "            elif position == (3,11):\n",
    "                output = \" T \"\n",
    "                #print(self.s)\n",
    "            elif self._cliff[position]:\n",
    "                output = \" C \"\n",
    "            else:\n",
    "                output = \" o \"\n",
    "\n",
    "            if position[1] == 0:\n",
    "                output = output.lstrip() \n",
    "            if position[1] == self.shape[1] - 1:\n",
    "                output = output.rstrip() \n",
    "                output += \"\\n\"\n",
    "\n",
    "            outfile.write(output)\n",
    "        outfile.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: We start in the state (3,0) indicated by an x and we need to find a \"good\" route to the T state.\n",
    "\n",
    "- <font color='dark-magenta'>TASK:</font> Inspect the source and make you understand the states, actions, rewards and transition probabilities. Hint: is this a stochastic environment?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can e.g. take a step and see what happends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "x  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  T\n",
      "\n",
      "24\n",
      "-1.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = env.step(0) # 0 is UP\n",
    "env.render()\n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='dark-magenta'>TASK:</font> <font color=\"red\">[descriptive]</font> What is the optimal policy (given that you have access to the full state-space and know where the target is)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we see that it is NOT a stochastic environment (however you still need to learn not to go over the cliff).We also observe that the starting state is (3,0), i.e. state (0,0) is the top right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can get a sense of the action and state representations by runnign the follwing std (Open AI) commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4)\n",
      "Discrete(48)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e. we have 4 discrete actions and 48 unique states (discrete)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of taking an taking in the space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "-1.0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "next_state, reward, done, _ = env.step(0)\n",
    "    \n",
    "print(next_state)\n",
    "print(reward)\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can easily see that the optimal policy (along the edge) is: up -> 11xleft -> 1xdown (i.e. 13 steps). Note had the env been stochastic this would have been different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9.2 Policy Search (review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='dark-magenta'>REVIEW</font>: Review policy search in the AIMA book (sec 21.5) or from the overview below:\n",
    "\n",
    "Recall that policy search requires the following:\n",
    "\n",
    "- A function which quantifies how good it is to take action in a state, let's call it $Q_{\\theta}(s, a)$ as in the AIMA book (others call it $h_{\\theta}(s, a)$)\n",
    "    - We will use a basic neural network defined in [tensorflow](https://www.tensorflow.org/) for defining this function approximation. The main reason for using tensorflow is that it provides us with the different types of neural network layers and can automatically provide the gradients (in the case of the policy !).\n",
    "    \n",
    "    \n",
    "- A way of mapping the $Q_{_\\theta}$ function to a policy to determine which actions to take in a specific action.\n",
    "    - We typically use the [softmax](https://en.wikipedia.org/wiki/Softmax_function) function for obtaining a differentiable policy, i.e:\n",
    "    $$\\pi (a|s,\\theta )= \\frac{{{e^{{Q_\\theta }(s,a)}}}}{{\\sum\\limits_{a'}^{} {{e^{{Q_\\theta }(s,a')}}} }}$$\n",
    "\n",
    "\n",
    "- The goodness of the policy \n",
    "    - We use the basic idea of the expected utility and define the value/goodness of a policy, $\\rho(\\theta)$, as the *true* expected sum of future rewards under that (stochastic) policy.\n",
    "    \n",
    "\n",
    "- A method to update the policy to maximize the goodness, i.e., $\\rho(\\theta)$, of the policy:\n",
    "    - It can seem challenging to change the policy defined by $\\theta$ to maximize $\\rho(\\theta)$ in a way that\n",
    "ensures improvement. Goodness depends on both the action choice and the distribution of states in which those actions are chosen. These are both dependent on $\\theta$. For a specific state the effect of $\\theta$ on the actions and reward, can be computed in a relatively easy way, however, the effect of $\\theta$ on the state distribution depends on the unknown environment. So the main question is: How can we estimate the performance gradient with respect to the policy parameter when the gradient depends on the unknown effect of policy changes on the state distribution?\n",
    "    - We rely on the policy gradient theorem which leads us to an estimate of the gradients after observing a single transition\n",
    "    $$\\theta  \\leftarrow \\theta  + \\alpha {G_t}\\frac{{\\nabla \\pi \\left( {{a_j}|{s_j},\\theta } \\right)}}{{\\pi \\left( {{a_j}|{s_j},\\theta } \\right)}}$$\n",
    "    where $G_t$ is the future sum of discounted returns from that state onwards (which can be computed after an episode has been completed).\n",
    "    Note: we can also estimate the gradient based on multiple transitions and do a batch update (as in the lecture slides and book).\n",
    " The gradient wrt to the individual parameters (i.e. a vector) is the direction in parameter space that most increases the probability of repeating the action $A_t$ on future\n",
    "visits to state $S_t$. We can see that the update increases the parameter vector in this direction proportional to $G_t$, and inversely proportional to the action probability, $\\pi(a_j|s_j,\\theta)$. The former makes sense because it causes the parameter to move most in the directions that favor actions that yield the highest return. The latter makes sense because otherwise actions that are selected frequently are at an advantage (the updates will be more often in their direction) and might win out even if they do not yield the highest return (AIMA Chapter 21 and Sutton & Barto: Reinforcement Learning: An Introduction, Chapter 13).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9.3 Policy Approximation (and its gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we will implement/define the policy appoximation/estimator in [tensorflow.](https://www.tensorflow.org/learn). The code is carefully annotated (to solve the tasks below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkPolicyEstimator():\n",
    "    \"\"\" \n",
    "    A very basic MLP neural network approximator and estimator for poliy search    \n",
    "    \n",
    "    The only tricky thing is the traning/loss function and the specific neural network arch\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha, n_actions, d_states, nn_config, verbose=False):        \n",
    "        self.alpha = alpha    \n",
    "        self.nn_config = nn_config                   \n",
    "        self.n_actions = n_actions        \n",
    "        self.d_states = d_states\n",
    "        self.verbose=verbose # Print debug information        \n",
    "        self.n_layers = len(nn_config)  # number of hidden layers        \n",
    "        self.model = []\n",
    "        self.__build_network(d_states, n_actions)\n",
    "        self.__build_train_fn()\n",
    "             \n",
    "\n",
    "    def __build_network(self, input_dim, output_dim):\n",
    "        \"\"\"Create a base network usig the Keras functional API\"\"\"\n",
    "        self.X = layers.Input(shape=(input_dim,))\n",
    "        net = self.X\n",
    "        for h_dim in nn_config:\n",
    "            net = layers.Dense(h_dim)(net)\n",
    "            net = layers.Activation(\"relu\")(net)\n",
    "        net = layers.Dense(output_dim, kernel_initializer=initializers.Zeros())(net)\n",
    "        net = layers.Activation(\"softmax\")(net)\n",
    "        self.model = Model(inputs=self.X, outputs=net)\n",
    "\n",
    "    def __build_train_fn(self):\n",
    "        \"\"\" Create a custom train function\n",
    "        It replaces `model.fit(X, y)` because we use the output of model and use it for training.        \n",
    "        Called using self.train_fn([state, action_one_hot, target])`\n",
    "        which would train the model. \n",
    "        \n",
    "        Hint: you can think of K. as np.\n",
    "        \n",
    "        \"\"\"\n",
    "        # predefine a few variables\n",
    "        action_onehot_placeholder   = K.placeholder(shape=(None, self.n_actions),name=\"action_onehot\") # define a variable\n",
    "        target                      = K.placeholder(shape=(None,), name=\"target\") # define a variable       \n",
    "        \n",
    "        # this part defines the loss and is very important!\n",
    "        action_prob        = self.model.output # the outlout of the neural network        \n",
    "        action_selected_prob        = K.sum(action_prob * action_onehot_placeholder, axis=1) # probability of the selcted action        \n",
    "        log_action_prob             = K.log(action_selected_prob) # take the log\n",
    "        loss = -log_action_prob * target # the loss we are trying to minimise\n",
    "        loss = K.mean(loss)\n",
    "        \n",
    "        # defining the speific optimizer to use\n",
    "        adam = optimizers.Adam(lr=self.alpha)# clipnorm=1000.0) # let's use a kereas optimiser called Adam\n",
    "        updates = adam.get_updates(params=self.model.trainable_weights,loss=loss) # what gradient updates to we parse to Adam\n",
    "            \n",
    "        # create a handle to the optimiser function    \n",
    "        self.train_fn = K.function(inputs=[self.model.input,action_onehot_placeholder,target],\n",
    "                                   outputs=[],\n",
    "                                   updates=updates) # return a function which, when called, takes a gradient step\n",
    "      \n",
    "    \n",
    "    def predict(self, s, a=None):              \n",
    "        if a==None:            \n",
    "            return self._predict_nn(s)\n",
    "        else:                        \n",
    "            return self._predict_nn(s)[a]\n",
    "        \n",
    "    def _predict_nn(self,state_hat):                          \n",
    "        \"\"\"\n",
    "        Implements a basic MLP with tanh activations except for the final layer (linear)               \n",
    "        \"\"\"                \n",
    "        x = self.model.predict(state_hat)                                                    \n",
    "        return x\n",
    "  \n",
    "    def update(self, states, actions, target):  \n",
    "        \"\"\"\n",
    "            states: a interger number repsenting the discrete state\n",
    "            actions: a interger number repsenting the discrete action\n",
    "            target: a real number representing the discount furture reward, reward to go\n",
    "            \n",
    "        \"\"\"\n",
    "        action_onehot = np_utils.to_categorical(actions, num_classes=self.n_actions) # encodes the state as one-hot\n",
    "        self.train_fn([states, action_onehot, target]) # call the custom optimiser which takes a gradient step\n",
    "        return \n",
    "        \n",
    "    def new_episode(self):        \n",
    "        self.t_episode  = 0.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "***Look through the annotated code above and address the following question.***\n",
    "\n",
    "- <font color='dark-magenta'>NOTE</font>:  Make sure you appreciate what one-hot-encoding means (look online if you are in doubt).\n",
    "- <font color='dark-magenta'>TASK</font>:  <font color=\"red\">[descriptive]</font> What does the 'target' in the code represent (from reading the code annotation/description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- target represents $G_t$ (the observed reward to go). This can be computed for any state in a observed sequance of states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='dark-magenta'>TASK</font>: <font color=\"red\">[descriptive]</font> What is the function approximator used (i.e. what is architecture of the neural network). Hint: it is a rather simple neural network but try to draw the structure anyway (you can extend the network at a later stage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "Assuming that the states are each described by a vector s=[s_0, s_1,.., s_K] the network can be visualized as (without bias terms):\n",
    "<img src=\"nn_full.png\" width='100%'>\n",
    "Where we have indicated the linear activation as a straight line. \n",
    "\n",
    "However, we encode the states as one-hot (e.g. [1,0,0,0]), if we consider $\\pi(a|s_0)$ we have:\n",
    "<img src=\"nn_s0.png\" width='100%'>\n",
    "This means that only the weigths related to the first input contributes becaues all other elements of the s vector is zero.\n",
    "\n",
    "I.e. the network is basically a look-up table with the values determined by the weights - but TF provides an easy way to find the gradients and update rules for the elements through the softmax function!\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div style=\"border:0.5px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- <font color='dark-magenta'>TASK</font>: <font color=\"red\">[descriptive]</font> What is the loss function (write it up as an equation). Assuming the specificed optimizer (Adam) can compute the gradient of the loss with respect to $\\theta$ (i.e. the weights); does this match the update equations in REINFOCE as specified in Q9.2 (hint: this is not an easy question an make sure you talk to the tutors or look over the answers when uploaded).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>\n",
    "\n",
    "Yes, it matches the REINFORCE updates.\n",
    "\n",
    "The loss is (from the code):\n",
    "\n",
    "\n",
    "`loss = -K.log(action_selected_prob) * target`\n",
    "\n",
    "\n",
    "I.e.\n",
    "\n",
    "$$Loss = - log( \\pi(a_j | s_j, \\theta) ) * G_t$$ \n",
    "\n",
    "We rely on the defined optimizer (Adam) to take the gradient step, but we can validate that this step will match the REINFORCE updates defined above. If we differentiale the defined loss with respect to $\\theta$ we get:\n",
    "\n",
    "$$\\frac{{\\partial {\\text{self}}{\\text{.loss}}}}{{\\partial \\theta }} = -{G_t}\\frac{{\\partial \\ln \\pi (a|s,\\theta )}}{{\\partial \\theta }} = -{G_t}\\frac{{\\tfrac{\\partial }{{\\partial \\theta }}\\pi (a|s,\\theta )}}{{\\pi (a|s,\\theta )}}$$\n",
    "\n",
    "where we exploit an identity $\\nabla_{\\theta} log z = (\\nabla_{\\theta} z) / z$. This matches the update in the REINFORCE updates noting that the optimizer (Adam) is a method for minimizing a function (i.e. the sign is also correct in this context).\n",
    "\n",
    "<br>\n",
    "\n",
    "<div style=\"border:0.5px solid red\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.3 REINFORCE - implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we setup the main loop of the REINFORCE method, i.e., the iterations over the episodes and steps etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, estimator_policy, num_episodes, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    REINFORCE (Monte Carlo Policy Gradient) Algorithm. Optimizes the policy\n",
    "    function approximator using policy gradient.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        estimator_policy: Policy Function to be optimized         \n",
    "        num_episodes: Number of episodes to run for\n",
    "        discount_factor: reward discount factor\n",
    "    \n",
    "    Returns:\n",
    "        An EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \n",
    "    Adapted from: https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/CliffWalk%20REINFORCE%20with%20Baseline%20Solution.ipynb\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    Transition = collections.namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset the environment and pick the fisrst action\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode = []\n",
    "        \n",
    "        # One step in the environment\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            state_oh = np.zeros((1,env.nS))\n",
    "            state_oh[0,state] = 1.0                                   \n",
    "            action_probs = estimator_policy.predict(state_oh)\n",
    "            action_probs = action_probs.squeeze()\n",
    "            \n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            ##\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Keep track of the transition\n",
    "            episode.append(Transition(\n",
    "              state=state, action=action, reward=reward, next_state=next_state, done=done))\n",
    "            \n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} @ Episode {}/{} ({})\".format(\n",
    "                    t, i_episode + 1, num_episodes, stats.episode_rewards[i_episode - 1]), end=\"\")            \n",
    "\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "        # Go through the episode, step-by-step and make policy updates (note we sometime use j for the individual steps)\n",
    "        estimator_policy.new_episode()\n",
    "        new_theta=[]\n",
    "        for t, transition in enumerate(episode):                 \n",
    "            state_oh = np.zeros((1,env.nS))\n",
    "            state_oh[0,transition.state] = 1.0\n",
    "            \n",
    "            # The return, G_t, after this timestep; this is the target for the PolicyEstimator\n",
    "            G_t = sum(discount_factor**i * t.reward for i, t in enumerate(episode[t:]))\n",
    "           \n",
    "            # Update our policy estimator\n",
    "            estimator_policy.update(state_oh, transition.action,np.array(G_t))            \n",
    "         \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- <font color='dark-magenta'>TASK</font>: <font color=\"red\">[pseudo-code]</font> Browse through the code and write down pseudo-code without all the implementation details which highlight the main loops in the programme (i.e. over episodes and steps). Specifically, identify how many transitions are actually parsed to the gradient-based optimizer each time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px solid red\"></div>\n",
    "<font color=\"red\">SOLUTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pseudo-code: We'll leave this to the reader.\n",
    "\n",
    "The main thing to notice is that the update is carried out based on one transition (in a given episode) only. In the book and slides we're typically computing the gradient over multiple transitions/steps (i.e. an average gradient) however updating based on only one step is perfectely valid in an online/stochastic gradient approach. Although fine-tuning the step size is sometimes more difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:0.5px solid red\"></div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
